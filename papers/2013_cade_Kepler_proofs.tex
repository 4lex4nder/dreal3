\documentclass[envcountsect]{llncs}
%\usepackage{stmaryrd,amsmath,amssymb,newlfont,graphicx,caption,verbatim}
\usepackage{amsmath,amssymb,newlfont,graphicx,caption,verbatim,hyperref}
\usepackage[ruled,lined,boxed,commentsnumbered,linesnumbered]{algorithm2e}
\usepackage{mathpartir}
%\usepackage{hyperref}

\newcommand{\Var}{\mathop{\mathit{Var}}}
\newcommand{\Env}{\mathop{\mathit{Env}}}
\newcommand{\dom}{\mathrm{dom}}
\newtheorem{notation}[theorem]{Notation}
\newcommand{\len}{\mathit{len}}
\newcommand{\poly}{\mathsf{poly}}
\newcommand{\ir}{\mathbb{IR}_{\cup}}

%\setlength{\textwidth}{5.1in}
%\setlength{\textheight}{8.2in}
%\setlength{\topmargin}{0in}
%\setlength{\oddsidemargin}{.7in}
%\setlength{\evensidemargin}{.7in}


\title{Extracting Proofs from a Numerically-Driven Decision Procedure}
\author{Sicun Gao \and Soonho Kong \and Michael Wang \and Edmund M. Clarke}
\institute{Carnegie Mellon University, Pittsburgh, PA 15213}

\begin{document}
\maketitle

\begin{abstract}
$\delta$-Complete decision procedures can solve SMT problems over the
reals with a wide range of nonlinear functions, allowing ``$\delta$-bounded
errors''. The scalability of such procedures usually depends on efficient
numerical procedures, whose implementation can be error-prone. It is important
for $\delta$-complete solvers to provide certificates to prove the correctness
of their answers. We show how to do this for DPLL$\langle$ICP$\rangle$, a
general solving framework based on Interval Constraint Propagation. We
focus on the construction of proof trees for the ``unsat''
answers and the proof-checking of their correctness. Besides certifying solvers,
we find our approach a promising one for automated theorem proving over
the reals, exploiting the power of numerical algorithms in a formal way. One
direct application is to establish many nonlinear lemmas in the Flyspeck
project for the formal proof of the Kepler Conjecture.
\end{abstract}

\section{Introduction}

SMT formulas over the real numbers can encode a wide range of problems in
theorem proving and formal verification. Such formulas are very hard to solve
when nonlinear functions are involved~\cite{}. Our recent work on
{\em $\delta$-complete decision procedures} provided a new general framework for
handling nonlinear SMT problems over the reals~\cite{}. We say a decision
procedure is {\em $\delta$-complete} for a set $S$ of SMT formulas, where
$\delta$
is any positive rational number, if for any $\varphi$ from $S$ the procedure
returns one of the following answers:
\begin{itemize}
 \item {\sf unsat}: $\varphi$ is unsatisfiable.
 \item {\sf $\delta$-sat}: $\varphi^{\delta}$ is satisfiable.
\end{itemize}
Here, $\varphi^{\delta}$ is a syntactic variation of $\varphi$ that encodes a
notion of numerical perturbation on logic formulas (more
details in Section \ref{review}). Essentially, we allow such a procedure to
give answers with one-sided, $\delta$-bounded errors. With this relaxation,
$\delta$-complete decision procedures can fully exploit the
power of scalable numerical algorithms to solve nonlinear
problems, and at the same time provide suitable correctness
guarantees for many correctness-critical problems~\cite{DBLP:conf/cade/GaoAC12}.

An important problem
for practical SMT solvers is that the correctness
of their answers should be verified. A standard approach is that, instead of
complete verification of the software programs, a solver should provide
certificates on-the-fly along with its answers. That is, when the solver
determines that a formula $\varphi(\vec x)$ is ``sat'', it produces an
assignment $\vec a$ to all the
variables such that the ground formula $\varphi(\vec a)$ is easily checked to be
true. On the other hand, when $\varphi(\vec x)$ is determined to be ``unsat'',
the solver can produce a proof $P$ that establishes the validity of $\forall
\vec x.\neg\varphi(\vec x)$ in a suitable proof system. Here, $P$ is called a
{\em proof of unsatisfiability}. In the framework of $\delta$-complete
decision procedures, obtaining certificates from numerically-driven SMT solvers
is especially important. Numerical algorithms usually contain complex heuristics
and floating-point operations, and it is very hard to perform static
verification on the programs directly. On the other hand, if the solutions
witnessing ``$\delta$-sat'' answers, and proofs of unsatisfiability for the
``unsat'' answers are extracted,
we can check their correctness as stand-alone using symbolic
arbitrary-precision computations. The inner mechanisms of the numerical
algorithms are not relevant in the certification process.

From this perspective, our technique can be seen as a new approach to the challenging
task of automated theorem proving over the reals. Note that the ``unsat''
answers never contains numerical errors. Such an approach
 would combine the best of two worlds: numerical procedures are
fast but error-prone, and are used as oracles for the scalable exploration of
the search space (either searching for a $\delta$-solution, or a proof of
unsatisfiability); symbolic algorithms are precise but slow, and are used for
validating the outcome certificates, which is a much less
computationally-intensive task.

In this paper, we show how to extract and validate such proofs
of correctness for numerically-driven SMT solvers that implement the
DPLL$\langle$ICP$\rangle$ algorithm~\cite{DBLP:conf/cade/GaoAC12}, for solving nonlinear formulas over
the reals. The challenge lies in extracting symbolic proofs of unsatisfiability
that do not carry over any possible numerical errors, and the complete
validation of them as theorems in a sound proof system (ideally, containing only
simple proof rules).

Interval Constraint Propagation (ICP)~\cite{Newton} is a branch-and-prune algorithm
for solving systems of real constraints, which acts as the theory solver in the
DPLL(T) framework. The algorithm maintains an interval
assignment to all the variables, and update the assignments based on their
consistency with the constraints. In a ``pruning''
step, ICP contracts the intervals by pruning away subintervals that do not
contain any solution; in a ``branching'' step, ICP subdivides an interval to
create subproblems and solve them recursively. The similarity between ICP and SAT solving techniques has
been explored in the work~\cite{}.

Our approach is as follows. First, we formalize the ICP algorithm in the
format of Abstract DPLL~\cite{DBLP:journals/jacm/NieuwenhuisOT06}, such that its computation corresponds to
sequences of abstract transitions. We then use a simple first-order proof
calculus $\mathbb{D}_A$, relativized to a set $A$ of axioms over the reals, and
show how to transform a run of the Abstract ICP to a proof in the system. Next,
we show how to validate the generated proofs using a stand-alone symbolic proof
checker. The proof checker only implements simple rules that are easy to verify,
and it interacts with the solver in an abstraction refinement loop to obtain
proof trees of sufficient detail. A main focus for our tool is to prove
nonlinear lemmas
in the Flyspeck project for the formal proof of the Kepler
conjecture~\cite{DBLP:journals/dcg/HalesHMNOZ10,DBLP:conf/dagstuhl/Hales05}. They
involve large numbers on nonlinear constraints including trigonometric functions. We show
promising experimental results towards the goal.

The paper is organized as follows. We review the framework of $\delta$-complete
decision procedures in Section~\ref{review}. We then show how to formalize ICP,
construct proofs, and proof-check them in Section~\ref{icp}. We
report experiments with lemmas in the Flyspeck project in Section~\ref{kepler}.

\subsubsection{Related Work} Our work is closely related to several lines
of research in the existing literature. For proving formulas with
transcendental functions,
MetiTarski~\cite{DBLP:conf/itp/Paulson12,DBLP:journals/jar/AkbarpourP10,DBLP:conf/aisc/PassmorePM12}
is the leading tool that reduces problems to
polynomials and calls quantifier elimination procedures. For problems with only
polynomials, Bernstein polynomials are used in PVS for formal
proofs~\cite{MN12}. Our approach can be seen as complementary to these
approaches. The iSAT solver~\cite{HySAT} also contains strategies for certifying
their answers, in a different framework. Some further comparisons are given in an extended version~\cite{cade2013extended}. There are now several SMT solvers~\cite{} for formulas with nonlinear polynomials over
the reals with no proof-producing capacities. Proofs for
correctness in general SMT solvers has been well studied in~\cite{DBLP:journals/fmsd/StumpORHT1}.

\section{A Brief Review of $\delta$-Complete Decision Procedures}\label{review}

We briefly review the framework of $\delta$-complete decision procedures.
It formulates a reasonable correctness requirement for decision
procedures for nonlinear
formulas over the reals, while the standard completeness requirement can not be
imposed because of intractability. We have shown that $\delta$-complete
procedures exist,
with reasonable complexity bounds, for bounded SMT formulas over the reals with
arbitrary Type-2 computable real functions (a notion well-studied in computable
analysis~\cite{}).

First, to formalize computations over the reals, we need to encode the
real numbers as infinite strings. We can then model
computations of real functions with machines that can use infinite tapes as
input and output. That is, a real function is computable if there exists a
machine that computes, using oracles that encode the arguments of the function,
the values of the function to an arbitrary precision. These
notions are captured by the following definitions.
\begin{definition}[Encoding Real Numbers]
A {\em name} of a real number $a\in \mathbb{R}$ is a function
$\mathcal{\gamma}_a: \mathbb{N}\rightarrow \mathbb{Q}$ satisfying that for all
$i\in \mathbb{N}$, $|\gamma_a(i) - a|<2^{-i}.$ For vectors $\vec a\in
\mathbb{R}^n$,
$\gamma_{\vec a}(i) = \langle \gamma_{a_1}(i), ..., \gamma_{a_n}(i)\rangle$.
Write $\Gamma(\vec a) = \{\gamma: \gamma\mbox{ is a name of }\vec a\}$.
\end{definition}
\begin{definition}[Computable Real Functions] We say
$f:\subseteq\mathbb{R}^n\rightarrow \mathbb{R}$ is computable, if there exists
an oracle Turing machine $\mathcal{M}_f$ as follows. Let $\vec a\in \dom(f)$
be any argument of $f$ and $\gamma(\vec a)$
any name of $\vec a$. On any input $i\in \mathbb{N}$,
$\mathcal{M}_f^{\gamma(\vec a)}(i)$ uses $\gamma(\vec a)$ as an oracle, and
computes a $2^{-i}$-approximation to $f(\vec a)$.
\end{definition}
Most common continuous real functions are computable, such as
arithmetic, absolute value, $\min$, $\max$, $\exp$, $\sin$ and solutions of
Lipschitz-continuous ordinary differential equations~\cite{CAbook}. Compositions
of computable functions are computable.

Now, suppose $\mathcal{F}$ denotes an arbitrary collection of computable real
functions. Let $\mathcal{L}_{\mathcal{F}} = \langle <,
\mathcal{F}\rangle$ be the first-order signature over the structure
$\mathbb{R}_{\mathcal{F}} = \langle
\mathbb{R}, \leq, \mathcal{F}\rangle$. (Constants are seen as 0-ary
functions in $\mathcal{F}$.) We can then consider SMT problems
over $\mathbb{R}_{\mathcal{F}}$, namely, satisfiability of quantifier-free
$\mathcal{L}_{\mathcal{F}}$-formulas over $\mathbb{R}_{\mathcal{F}}$. We
consider bounded SMT problems, which is more conveniently expressed as
$\Sigma_1$-sentences with bounded quantifiers as follows. We say a
$\Sigma_1$-sentence is bounded, if it can be written in the form
$$\varphi:\ \exists^{I_1}x_1\cdots \exists^{I_n}x_n. \psi(x_1,...,x_n)$$
where: for all $i$, $I_i\subseteq \mathbb{R}$ is a bounded (open or closed)
interval; each bounded quantifier $\exists^{I_i}x_i.\phi$ denotes $\exists
x_i.(x_i\in I_i\wedge \phi)$. $\psi(x_1,...,x_n)$ is a quantifier-free
$\mathcal{L}_{\mathcal{F}}$-formula, i.e., a Boolean combination of atomic
formulas $f(x_1,...,x_n)\circ 0$, where $f$ is a composition of
functions in $\mathcal{F}$ and $\circ\in\{<,\leq, >, \geq, =, \neq \}$.
All the functions occurring in $\psi(\vec x)$ should be defined everywhere over
the closure of $I_1\times\cdots \times I_n$. Any bounded $\Sigma_1$-sentence
can be put into the following standard form, where inequalities are implicitly
expressed by the bounds on quantifiers (using slack variables) and the
atomic formulas only involve equalities:
\begin{proposition}[Standard Form]\label{pre1}
Any bounded $\Sigma_1$-sentence $\varphi$ in $\mathcal{L}_{\mathcal{F}}$ is
equivalent over $\mathbb{R}_{\mathcal{F}}$ to a sentence $\exists^{I_1}x_1\cdots
\exists^{I_n}x_n(\bigwedge_{i=1}^m(\bigvee_{j=1}^{k_i}
f_{ij}(\vec x)=0)).$
\end{proposition}
We can now define the notion of ``$\delta$-perturbations'' on these
formulas:
\begin{definition}[$\delta$-Weakening and Perturbations]\label{weak-def}
Let $\delta\in \mathbb{Q}^+\cup\{0\}$ be a constant and $\varphi$ be a
$\Sigma_1$-sentence in standard form:
%\vspace{-.3cm}
\[\varphi:= \exists^{\vec I}\vec x\;(\bigwedge_{i=1}^m (\bigvee_{j=1}^{k_i}
f_{ij}(\vec x)= 0)).
%\vspace{-.3cm}
\]
The $\delta$-perturbed form (or $\delta$-weakening) of $\varphi$ defined as:
%\vspace{-.3cm}
\[\varphi^{\delta}:= \exists^{\vec I} \vec x\;(\bigwedge_{i=1}^m(\bigvee_{j=1}^k
|f_{ij}(\vec x)|\leq \delta)).\]
Also, a $\delta$-perturbation is a constant vector $\vec c =
(c_{11},...,c_{mk_m})$, where $c_{ij}\in\mathbb{R}$ and $||\vec
c||_{\infty}\leq\delta$, such that $\varphi^{\vec c}:= \exists^{\vec I} \vec
x\;(\bigwedge_{i=1}^m(\bigvee_{j=1}^k
f_{ij}(\vec x) = c_{ij})).$
\end{definition}

\begin{definition}[Bounded $\delta$-SMT in $\mathcal{L}_{\mathcal{F}}$] Let
$\mathcal{F}$ be a finite collection of Type 2 computable functions. Let
$\varphi$ be a bounded $\Sigma_1$-sentence in $\mathcal{L}_{\mathcal{F}}$ in
standard form. The {\em bounded $\delta$-SMT problem} asks for one of the
following two decisions on $\varphi$:
\begin{itemize}
\item $\mathsf{unsat}:$ $\varphi$ is false.
\item $\delta$-$\mathsf{sat}:$ $\varphi^{\delta}$ is true.
\end{itemize}
When the two cases overlap, either decision can be returned.
\end{definition}

The main theoretical result is that for any positive
$\delta$, the bounded $\delta$-SMT problems in $\mathcal{L}_{\mathcal{F}}$ are
decidable (namely, $\delta$-decision
procedures exist).
\begin{theorem}[Decidability] Let $\mathcal{F}$ be a finite collection
of computable real functions and $\delta\in \mathbb{Q}^+$. The bounded
$\delta$-SMT problem in $\mathcal{L}_{\mathcal{F}}$ is decidable.
\end{theorem}

The $\delta$-SMT problems also have reasonable complexity bounds for various
signatures that would otherwise define undecidable theories.
\begin{theorem}[Complexity]
Let $\mathcal{F}$ be a finite collection of functions in Type 2 complexity class
$\mathsf{C}$, $\mathsf{P}\subseteq\mathsf{C}\subseteq\mathsf{PSPACE}$. The
$\delta$-SMT problem for uniformly bounded $\Sigma_1$-classes in
$\mathcal{L}_{\mathcal{F}}$ is in $\mathsf{NP^C}$. For instance, when
$\mathcal{F}$ only contains {\sf P}-time computable real functions such as
$\{+, \times, \exp, \sin\}$, the problem is $\mathsf{NP}$-complete.
\end{theorem}

These results lead to a
new perspective on decision problems over the reals in general.
This framework provides a theoretical basis for the development of
numerically-driven decision procedures such as~\cite{}.

\section{Extracting Proofs from Interval Constraint Propagation}\label{icp}

\subsection{Formalizing Interval Constraint Propagation}

Interval Constraint Propagation (ICP)~\cite{handbookICP} finds
solutions of real constraints using the ``branch-and-prune'' method, combining
interval arithmetic and constraint propagation. The idea is to use interval
extensions of functions to ``prune'' out sets of points that are not in the
solution set and ``branch'' on intervals when
such pruning can not be done, until a small enough box that may
contain a solution is found, or report inconsistency otherwise. More details
can be found in the Appendix and~\cite{}.
\subsubsection{Abstract ICP} Our task now is to formalize ICP algorithms
so that we can extract symbolic proofs from its computation process. The
branch-and-prune structure of ICP is very similar to DPLL-based SAT solving, so
we follow the format of Abstract DPLL~\cite{}. We will represent ICP
as a transition system, whose states consist of interval assignments and the
real constraints to be solved.

An interval $I$ is any connected subset of
$\mathbb{R}$, and we write $\mathbb{IR}$ to denote the set of all the intervals.
We first formalize how ICP maintains interval assignments to a set of variables
as follows.
\begin{definition}[Interval Assignment Sequence]
Let $x_1,...,x_n$ be real variables. An {\em interval assignment sequence} over
$\vec x$ is a sequence $(s_1,...,s_m)$, where
$$s_i\in \{(x_i\in I_j): 1\leq i\leq n, I_j\in
\mathbb{IR}_{\cup}\}\cup\{(x_i\in I_j)^d: 1\leq i\leq n, I_j\in
\mathbb{IR}_{\cup}\}.
$$
We write $(S_1, S_2)$ to denote the concatenation of two sequences $S_1$ and
$S_2$. The parentheses can be omitted when appropriate.
\end{definition}
It will be clear later that when we write $(x\in I)^d$, it means an arbitrary
choice on the value of $x$, which is consequently a backtrack point.
\begin{remark}
ICP can maintain unions of intervals for variables. In principle this is not
needed if we only consider the decision problem, which only searches for
one solution and the components of a union can be tested sequentially. So we
assume that only connected subsets of values are used here.
\end{remark}

\begin{definition}[Box Domain]
Let $S$ be an interval assignment sequence over variables $x_1,...,x_n$. The
{\em box
domain} associated with $S$ is defined by
$$\beta(S) = I_1\times\cdots \times I_n, \mbox{ where }I_i = \bigcap\{ I:
(x_i\in
I)\mbox{ or } (x_i\in I)^d \mbox{ occurs in } S\}.$$
Also, we write $\beta(S)_i$ to denote $I_i$.
\end{definition}

\begin{definition}[ICP Transitions]~\label{transitions} Let $\vec x =
(x_1,...,x_n)$ be a vector real variables. We write $c(\vec x)$ to denote an
arbitrary constraint over $\mathbb{R}^n$, and
$S$ an interval assignment sequence over $\vec x$. Let $S\parallel c$ be the
current state. We will always write $\beta(S_i) = I_i$ to denote the current
interval assignment on variable $x_i$. We now define the following transition
rules from $S\parallel c$ to another state.
\paragraph{(Pruning)} Let $I_i^1$ be a subset of $I_i$ such that $\forall \vec
a\in \beta(S,x_i\in I_i^1)$, $c(\vec a)$ is false. Then, if we let $I_i^2$ be an
interval satisfying $I_i\subseteq I_i^1 \cup I_i^2$, then
\begin{eqnarray*}
S\parallel c &\stackrel{p}{\Longrightarrow}& S, (x_i\in I_i^2)\parallel c
\end{eqnarray*}
is called a pruning step.
\paragraph{(Branching)}Let $I_i^1$ be a subset of $I_i$. Then
\begin{eqnarray*}
S\parallel c &\stackrel{br}{\Longrightarrow}& S, (x_i\in I_i^1)^d \parallel c,
\end{eqnarray*}
is called a branching step.

\paragraph{(Backtracking)} Let $I_i^1$ be a subset of $I_i$, such that $\forall
\vec a\in \beta(S,x_i\in I_i^1, S')$, $c(\vec a)$ is false. Let $I_i^2$ be an
interval such that $I\subseteq I_i^1\cup I_i^2$. If in addition, $S'$ does not
contain any $d$-assignment (anything of the form $(x\in I)^d$), then we can make
a transition
\begin{eqnarray*}
S, (x_i\in I_i^1)^d, S'\parallel c \stackrel{bt}{\Longrightarrow}& S, (x_i\in
 I_i^2) \parallel c,
\end{eqnarray*}
which is called a backtracking step.

\paragraph{(Failure)} Suppose $\forall \vec a\in \beta(S)$, $c(\vec a)$ is
false, and there is no $d$-assignment in $S$. Then we can make the transition
\begin{eqnarray*}
S\parallel c \stackrel{f}{\Longrightarrow} \emptyset\parallel c
\end{eqnarray*}
which is called a failure step.
\end{definition}
\begin{remark}
The main difference between the Abstract ICP and Abstract DPLL
is  that the assignments are not starting from empty, but contracted, and the
interval assignments on variables can be nondeterministic.
\end{remark}
\begin{definition}[Abstract ICP]
An $n$-dimensional ICP framework is a transition system
$$\langle \mathbb{IR}^n, \mathcal{S}, \mathcal{C}, \Longrightarrow,
\varepsilon\rangle$$
where $\mathcal{S}$ is the set of all interval assignment sequences over
$\mathbb{IR}^n$, and $\mathcal{C}$ is any set of constraints over
$\mathbb{R}^n$. A state is an element in $\mathcal{S}\parallel \mathcal{C}$. The
transition rules $\Longrightarrow: \mathcal{S}\times \mathcal{C}\rightarrow
\mathcal{S} \times \mathcal{C}$ are as specified in
Definition~\ref{transitions}. $\varepsilon\in \mathbb{Q}^+$ is an
error bound. A {\em run} of ICP is any sequence
$$S_1\parallel c, ... , S_k\parallel c,$$
where either $S_k$ is $\emptyset$, or $S_k\neq \emptyset$ and
$||\beta(S_k)||<\varepsilon$.
\end{definition}
\begin{remark}
We have defined ICP in a general way, without enforcing conditions
on the pruning operators, such as well-definedness as in~\cite{}. Thus, many
invalid ICP runs can be generated. In this way, we treat ICP as a proof
searching algorithm, and rely on the proof checkers to determine the correctness
of an ICP run. In practice, of course, only ``correct" ICP algorithms can
provide proofs that can always be validated.
\end{remark}
\begin{example}~\label{icp-example}
Consider a constraint $c(x,y) : y=x \wedge y = x^2$, and $x\in [1.5,2]$ and
$y\in [1,2]$ are the initial interval assignment. A possible ICP run is:
\begin{eqnarray*}
x\in [1.5,2], y\in [1,2]\parallel c &\stackrel{br}{\Longrightarrow}& x\in [1,2],
y\in [1,2], (x\in [1.7, 2])^d\parallel c\\
&\stackrel{bt}{\Longrightarrow}& x\in [1,2], y\in [1,2], x\in [1.5, 1.7]\\
& & \mbox{  (backtracking, since $\forall\vec a\in [1.7,2]\times [1,2]$,
$c(\vec a)$ is false,}\\
& & \ \ \mbox{and $[1.5,2]\subseteq[1.5,1.7]\cup [1.5, 2]$ for
$x$)}\\
&\stackrel{p}{\Longrightarrow}& x\in [1,2], y\in [1,2], x\in [1.5, 1.7], x\in
[1.5, 1.6]\parallel c\\
& & \mbox{  (pruning, since $\forall \vec a\in[1.6,1.7]\times [1, 2]$, $c(\vec
a)$ is false)}\\
&\stackrel{p}{\Longrightarrow}& x\in [1,2], y\in [1,2], x\in [1.5, 1.7],
x\in [1.5, 1.6], x\in \emptyset\parallel c\\
& & \mbox{  (pruning, since $\forall \vec a\in[1.5,1.6]\times [1, 2]$, $c(\vec
a)$
is false)}\\
&\stackrel{f}{\Longrightarrow}& \emptyset||c\ \mbox{ (since
$\forall \vec a\in \empty\times [1, 2], c(\vec a)$ is false.)}
\end{eqnarray*}
\end{example}




\subsection{First-Order Proofs of Unsatisfiability}

We focus on the proof the unsatisfiability of conjunctions of theory
atoms in the DPLL(T) framework, i.e., formulas of the form
$$\exists^{I_1} x_1\cdots \exists^{I_n} x_n.\; \bigwedge_{i=1}^m
f_i(x_1,...,x_n)\sim 0$$
where $\sim \in \{=,\neq, >, \geq, <, \leq\}$. It is clear that once such
proofs are obtained, the proof of unsatisfiability of Boolean combinations of
the theory atoms can be obtained, by simply plugging them in the high level
resolution proof. Also, it is important to note that the ICP algorithm solves
{\em systems} of constraints, and it regards the conjunction $\bigwedge_{i=1}^m
f_i(x_1,...,x_n)\sim 0$ as one constraint $c(x_1,...,x_n)$. Consequently, our
task is now reduced to obtaining proofs for the validity of formulas of the form
$\forall x_1 \cdots \forall x_n (x_1\in I_1\wedge \cdots \wedge x_n\in I_n
\rightarrow c(\vec x))$, from the failure of an ICP
search for a solution to its negation.

We will construct a simple first-order proof calculus, and show how to
transform ICP runs into proofs in the system.

Again, we consider formulas in a signature $\mathcal{L}_F = \langle <,
\mathcal{F} \rangle$, where constants are considered as 0-ary functions in
$\mathcal{F}$. When we write $x\in I$, where $I$ denotes an
interval, it is regarded as an abbreviation for the evident
equivalent $\mathcal{L}_\mathcal{F}$-formula. Note that this means that $I$ only
uses $\mathcal{L}_{\mathcal{F}}$-terms as end-points. Also, as mentioned above,
$c(\vec x)$ abbreviates a conjunction of atomic formulas. We also allow the use
of vectors in the formulas, writing $\vec x\in \vec I$ to denote $\bigwedge_i
x_i\in I_i$.

\begin{definition}[System $\mathbb{D}_A$]
We define $\mathbb{D}_A$ to be the first-order proof system consisting of only the
following two rules:
  \begin{mathpar}
    \inferrule{
      \forall \vec x (\psi \rightarrow \varphi)
      \and
    \forall \vec x (\psi' \rightarrow \varphi)
    }
  {
  \forall \vec x ( \psi\vee \psi' \rightarrow \varphi)
  }{\ \ \vee I}\and
  \inferrule{
  \forall x (\psi \rightarrow \varphi)
    \and
  \forall x (\psi' \rightarrow \psi)
  }
  {
  \forall x (\psi' \rightarrow \varphi)
  }{\ \ \forall\mbox{MP}}
  \end{mathpar}
and a set $A$ of axioms of the following two types:
\paragraph{Interval Axioms}
\begin{mathpar}
\inferrule{ }{\forall \vec x (\vec x\in \vec I \rightarrow \vec x\in \vec I_1
\vee \vec x\in \vec I_2 )}{\ \mbox{IA}}
\end{mathpar}
\paragraph{Constraint Axioms}
\begin{mathpar}
\inferrule{ }{\forall \vec x ( \vec x\in\vec I \rightarrow c(\vec x))}{\
\mbox{CA}}
\end{mathpar}
\end{definition}
Derivations in $\mathbb{D}_A$ are as standardly defined, as natural deductions
following these rules. Clearly, the two first-order rules are valid. Thus,
if all the axioms in $A$ are valid, then the system only produces valid
formulas over $\mathbb{R}$.
\begin{proposition}[Soundness] If $\mathbb{D}_A\vdash \varphi$ and
$\mathbb{R}\models \bigwedge A$, then $\mathbb{R}\models \varphi$.
\end{proposition}
\begin{remark}
 Clearly, the constraint axioms are the most nontrivial part. They are the
basic facts of real functions that a numerical procedure relies on, usually
concerning the range of functions within a small interval. The interval axioms
are sometimes not trivial either (for instance, compare intervals ending with
$e^{\pi}$ and $\pi^e$ respectively).  Proof-checking involves validation of
these axioms, which we discuss in Section~\ref{validate}.
\end{remark}

We now describe the construction of proof trees from ICP runs, which will be
represented as labeled binary trees. A labeled binary tree is defined as a
tuple $T =
\langle V, V_L,  \Sigma, \delta, \sigma\rangle$. Here, $V = \{v_0, ..., v_k\}$,
$k\in \mathbb{N}$, is a finite set of nodes, where $v_0\in V$ always denotes the
root
node. $\Sigma$ is a set of labels, which in our case is the set of
$\mathcal{L}_\mathcal{F}$-formulas. $\delta:\subseteq V\times \{l,r\}
\rightarrow V$ is a
partial mapping from a node to its descendant nodes, where $\delta(v, l)$ and
$\delta(v, r)$ denote the left and right descendant nodes, respectively.
$\sigma: V\rightarrow \Sigma$ is a labeling function that maps each node
$v\in V$ to a formula $\sigma(v) \in \Sigma$. In addition, the edges in the
tree can be labeled as well, through a function $\tau: V\times V\times \Omega$
where $\Omega$ is a set of edge-labels.

\subsubsection{Tree Generation} Let an ICP run be
$$S_0\parallel c\stackrel{t_1}{\Longrightarrow}\cdots
\stackrel{t_m}{\Longrightarrow} S_m\parallel c,$$
such that the ending transition $t_m$ is a failure step, i.e., $S_m=\emptyset$.
We now define the procedure by defining the functions $\delta$ and
$V_L$ through induction on $s_i$. Clearly the edges can be labeled with
$\Omega$ = \{{$\vee$I}, $\forall$M, IA, CA\}.

\paragraph{Case $i= 0$.} We label the root node $v_0$ by
$$\sigma(v_0) := \forall \vec x( \vec x\in \beta(S_0) \rightarrow \neg c).$$
Let $V_L^0= \{\delta(v_0,l), \delta(v_0, r)\}$ denote the current
collection of leaf nodes. Note that this formula is the negation of the input
SMT formula.


%{\tt it's
%probably better to define a leaf set and make the children mappings partial.
%Otherwise it's cumbersome to talk about the leaves and the ones without
%assigned formulas. }


\paragraph{Case $i = k+1$ ($1< k \leq m$). }
Suppose $V_L^k$ and $\sigma$ have been defined for $s_1, ...,s_k$. Write
$s_k = S_k
\parallel c$ and $s_k = S_{k+1} \parallel c$. Now we split
the cases on the type of the step $t$ from $s_i$ to $s_{i+1}$ as follows.
Again, we use the convention that $\beta(S)_i = I_i$ denotes the current
interval assignment on a variable $x_i$.
\paragraph{(Pruning Case)} Suppose $s_k\Longrightarrow s_{k+1}$ is a
pruning step. That is,
$$S_k\parallel c \stackrel{p}{\Longrightarrow} S_k, (x_i\in I_i^2)\parallel c,$$
where $I_i\subseteq I_i^1\cup I_i^2$ and $\forall \vec
a\in \beta(S_k,x_i\in I_i^1)$, $c(\vec a)$ is false. If we write
$$\vec I_1 = \beta(S_k, (x_i\in I_i^1)), \vec I_2 = \beta(S_k, (x_i\in
I_i^2)), \mbox{ and } \vec I= \beta(S_k),$$ then this step corresponds to
the following sub-tree of the proof
{\small
\begin{mathpar}
\inferrule{\inferrule{
  \inferrule{\vdots}{\forall \vec x (\vec x\in \vec I_2 \rightarrow \neg c)}
    \and
    \inferrule{\ }
    {
      \forall \vec x (\vec x \in \vec I_1 \rightarrow\neg c)
      }\mbox{CA}
   }
  {
  \forall x ( (\vec x\in \vec I_1\vee \vec x \in \vec I_2) \rightarrow \neg c)
  }\mbox{$\vee$I}
  \and
  {
  \inferrule{\ }{\forall x ( x\in I_i\rightarrow(x \in
I_i^1 \vee x\in I_i^2))}\mbox{IA}
  }
  }
{
\forall \vec x (\vec x\in\vec I \rightarrow \neg c)
}\mbox{$\forall$M}
\end{mathpar}}Formally, the sub-tree is added as follows. Let $v\in V_L^k$ be an
existing
leaf node that is labeled by the formula corresponding to $S_k\parallel c$;
namely,
$$\sigma(v) = \forall \vec x (\vec x\in\vec I \rightarrow \neg c).$$ (We
will inductively prove that such a node exists.) We then define
\begin{eqnarray*}
\delta(v, l) &=& v_{k+1}^1, \sigma(v_{k+1}^1) = \forall \vec x
( (\vec x \in \vec I_1 \vee \vec x \in \vec I_2) \rightarrow \neg c); \\
\delta(v, r) &=& v_{k+1}^2, \sigma(v_{k+1}^2) = \forall \vec x ( \vec x\in \vec
I\rightarrow( \vec x \in \vec I_1 \vee \vec x\in \vec I_2));\\
 \delta(v_{k+1}^1, l) &=& v_{k+1}^3, \sigma(v_{k+1}^3) = \forall \vec x (\vec x
\in \vec I_2 \rightarrow \neg c)\\
 \delta(v_{k+1}^1, r) &=& v_{k+1}^4, \sigma(v_{k+1}^4) = \forall \vec x (\vec x
\in \vec I_1 \rightarrow\neg c)
  \end{eqnarray*}
and set $V_L^{k+1} = (V_L^k \setminus\{v\})\cup \{v_{k+1}^2\}$.

\paragraph{(Branching Case)}Suppose $s_k\Longrightarrow s_{k+1}$ is a
branching step. That is,
\begin{eqnarray*}
S_k\parallel c &\stackrel{br}{\Longrightarrow}& S_k, (x_i\in I_i^1)^d \parallel
c,
\end{eqnarray*}
under the condition that $I_i^1\subseteq I_i$. If we write
$$\vec I_1 = \beta(S, (x_i\in I_i^1)), \vec I_2 = \beta(S, (x_i\in
I_i^2)), \mbox{ and } \vec I= \beta(S),$$ where $I\subseteq I_i^1\cup I_2$, then
this step corresponds to the following sub-tree:
{\small
\begin{mathpar}
\inferrule{\inferrule{
  \inferrule{\vdots}{\forall \vec x (\vec x\in \vec I_1 \rightarrow \neg c)}
    \and
    \inferrule{\vdots }
    {
      \forall \vec x (\vec x \in \vec I_2 \rightarrow\neg c)
      }
   }
  {
  \forall x (\vec x\in \vec I_1\vee \vec x \in \vec I_2 \rightarrow \neg c)
  }\mbox{$\vee$I}
  \and
  {
  \inferrule{\ }{\forall x ( x\in I_i\rightarrow(x \in
I_i^1 \vee x\in I_i^2))}\mbox{IA}
  }
  }
{
\forall \vec x (\vec x\in\vec I \rightarrow \neg c)
}\mbox{$\forall$M}
   \end{mathpar}
}Formally it is defined as follows. Again, let $v\in V_L^k$
be a leaf node such that $\sigma(v) = \forall \vec x (\vec x\in\vec I
\rightarrow \neg c).$ We then define
\begin{eqnarray*}
\delta(v, l) &=& v_{k+1}^1, \sigma(v_{k+1}^1) = \forall \vec x
( \vec x \in \vec I_1 \vee \vec x \in \vec I_2 \rightarrow \neg c); \\
\delta(v, r) &=& v_{k+1}^2, \sigma(v_{k+1}^2) = \forall \vec x ( \vec x\in \vec
I\rightarrow(\vec x \in \vec I_1 \vee \vec x\in \vec I_2));\\
 \delta(v_{k+1}^1, l) &=& v_{k+1}^3, \sigma(v_{k+1}^3) = \forall \vec x (\vec
x\in
\vec I_1 \rightarrow \neg c)\\
 \delta(v_{k+1}^1, r) &=& v_{k+1}^4, \sigma(v_{k+1}^4) = \forall \vec x (\vec x
\in \vec I_2 \rightarrow\neg c)
  \end{eqnarray*}
and set $V_L^{k+1} = (V_L^k\setminus \{v\}) \cup \{v_{k+1}^3, v_{k+1}^4 \}$.

\paragraph{(Backtracking Case)}Suppose $s_k\Longrightarrow s_{k+1}$ is
a branching step. That is,
\begin{eqnarray*}
S_{k'} , (x_i\in I_i)^d, S'\parallel c &\stackrel{bt}{\Longrightarrow}& S_{k'},
(x_i\in I_i^2 ) \parallel c,
\end{eqnarray*}
when $\forall a\in \beta(S, (x_i\in I_i)^d, S')$, $c(\vec
a)$ is false, and $I_i\subseteq I_i^2\cup I_i^1$, where $I_i =
\beta(S_{k'})_i$. $S_{k'}$ is a previous interval assignment
sequence, with $k'<k$. If we write
$$\vec I_1 = \beta(S, (x_i\in I_i^1)), \vec I_2 = \beta(S, (x_i\in
I_i^2), \mbox{ and } \vec I= \beta(S_{k'}),$$ then this step corresponds to the
following sub-tree of the proof.
{\small
\begin{mathpar}
\inferrule{
  \inferrule{
    \inferrule*{
      \inferrule*[vdots=1.5em, Right=\mbox{CA}]{ }{
        \forall \vec x
        (\vec x\in \beta(S_{k'},(x\in I_i^1)^d,S')\rightarrow \neg c)
      }
    }
    {\forall \vec x (\vec x\in \vec I_1
      \rightarrow \neg c)}
    \and
    \inferrule{
      \vdots
      }
    {
      \forall \vec x (\vec x \in \vec I_2 \rightarrow\neg c)
    }
   }
  {
  \forall x (\vec x\in \vec I_1\vee \vec x \in \vec I_2 \rightarrow \neg c)
  }\mbox{$\vee$-I}
  \and
  {
\vdots\ \ \
  }
  }
{
\forall \vec x (\vec x\in\vec I \rightarrow \neg c)
}\mbox{$\forall$-MP}
   \end{mathpar}
}Formally, we simply set $V_L^{k+1} = V_L^{k}$, and do not update the labeling.

\paragraph{(Fail Case)} Suppose it is a failure step. That is,
\begin{eqnarray*}
S\parallel c &\stackrel{f}{\Longrightarrow}& \emptyset \parallel c
\end{eqnarray*}
when $\forall \vec a\in \beta(S)$, $c(\vec a)$ is false and
 $S$ has no $d$-assignments. Let $\vec I
=\beta(S)$. This step corresponds to
{\small\begin{mathpar}
 \inferrule{\ }{\forall \vec x ( \vec x\in \vec I) \rightarrow \neg
c}\mbox{FA}
\end{mathpar}}We set $V_L^{k+1}=V_L^k$ and do not update $\sigma$.

\paragraph{Complete tree.} In all, after all the steps in the ICP run are
followed, the tree that we construct is $ T = \langle V, V_L^m, \delta, \sigma
\rangle$. The axiom set is given by $$A = \{\sigma(v): v\in V_L^m\}.$$

As an example, the transformation of the ICP run in Example~\ref{icp-example} is
shown in the Appendix.

It is easy to see that $T$ is a valid proof tree in $\mathbb{D}_A$.
\begin{proposition}\label{successful_tree}
For every ICP run ending with $\emptyset\parallel c$, the tree construction
procedure above produces a valid natural deduction tree in $\mathbb{D}_A$.
\end{proposition}
 The proof is contained in an extended report~\cite{}.
\begin{proposition}
 The size of the proofs is linear in the computation steps, which can be
exponential in the size of the problems.
\end{proposition}

Again, once the proof tree is constructed, the details of
the ICP algorithm no longer matters.  The only rules involved are the two
first-order rules in $\mathbb{D}_{A}$. Following relative soundness of the
system, to establish validity of the formula, we only need to validate the
axiom set $A$.

\subsection{Validating the Axioms}\label{validate}

There are two types of axioms that we allow in the proofs constructed from ICP
runs: interval axioms and constraint axioms. To validate such axioms, we still
need numerical computations. However, we will show that they only rely on
simple computations that can be validated through stand-alone
arbitrary-precision or symbolic computation. Note that the
validation of the axioms can fail -- they can fail even when the solver
correctly returns {\sf unsat}, if the solver uses advanced numerical
heuristics leading to axioms that can not be verified by reliable
numerical computation. In practice, we ensure the correctness of the proof
checker first, and use an abstraction refinement loop that allows the proof
checker to ask for more detailed proofs from the solver.

The interval axioms do not contain any real functions, and are of the form
$\forall \vec x(x\in I_1\vee x\in I_2\rightarrow x\in I)$. We only need to
check that $I$ is a subset of $I_1\cup I_2$ by comparing the end points of the
intervals, which is an easy numerical task.

The constraint axioms are of the form $\forall x (\vec x\in \vec I \rightarrow
c(\vec x))$, and can only be verified by considering the functions that
occur in $c$. Although they are of the same form as the formulas we solve,
these axioms should contain evident properties of the functions involved,
usually on small intervals. Such facts can be verified using reliable interval
computations, for instance as follows.
\begin{definition}[Interval Extensions]
Let $f: \mathbb{R}^n\rightarrow \mathbb{R}$ be a real function. An interval
function $F: \mathbb{IR}^n \rightarrow \mathbb{IR}$ is a function that
satisfies:
$$\forall I\in \dom(F), \{f(x): x\in I\}\subseteq F(I).$$
\end{definition}
A simple example of interval extensions is the {\em natural interval
extension}~\cite{} for
arithmetic operations, based on computations of functions on the end points of
intervals.

\begin{proposition}
Let $F$ be an interval extension of $f$, and $I\subseteq \dom(f)$. If
$F(I)\subseteq A$, then $\forall x (x\in I \rightarrow f(x)\in A)$.
\end{proposition}
Thus, the axioms are validated if we can verify that they are consistent with
all the interval extensions.
\begin{example}
The second pruning step in Example~\ref{icp-example} generates an axiom
$$\forall
x \forall y ( x\in [1.7, 2] \wedge y\in [1, 2] \rightarrow \neg (y=
x^2)\vee \neg (y=x))$$
This can be easily validated through the natural interval extension of
$(y-x^2)$, which is $[1,2]-[1.7,2]^2 = [-3, -0.89]$ and does not contain $0$.
\end{example}

In practice, ICP usually implements complicated heuristics that are more
powerful than what can be verified through direct interval arithmetic. In
principle, we could simulate the numerical heuristics to a certain extent with
reliable computations. However, a more practical approach first is to use an
abstraction refinement loop that allows the proof checker to ask the
solver for proof traces of the right amount of details. We call this the
``Branch and Prove'' loop.

 When we fail to prove an axiom through simple interval arithmetic, the proof
checker generates new subproblems that are returned to the solver. At this
stage, the axioms becomes the new theorems to be proved.
This is an abstraction refinement procedure, by executing the loop, we obtain
proof trees that contain more and more detailed steps, which may in the end be
completely verified by the proof checker using reliable computations.

There are two ways that the prover can generate the subproblems. One is to
branch on a variable in the formula, so that the solver can solve the same
problem on a smaller domain. The other option is to have the solver to run the
same problem, but with a smaller $\delta$. Note that under the condition that
the pruning operators in the solver is
well-defined~\cite{}, both procedures never change the
{\sf unsat} result. The branching may give
exponentially many new problems; while the $\delta$-change does not give new
problems, but may exponentially slow down the solver in each round. In practice
we observe that such a refinement loop is very useful, as we will show in the
experiments.

\section{Case Study: Proving Lemmas for the Kepler Conjecture}\label{kepler}

We build the proof-producing capacity into our open-source tool
dReal\footnote{Link to the
tool page and benchmarks is on our homepages,
\url{http://www.cs.cmu.edu/~sicung} and
\url{http://www.cs.cmu.edu/~soonhok}}. All the experiments below are ran on a
machine of with a 32-core 2.3GHz AMD Opteron Processor and 94GB of RAM.


\input{good_proofs.tex}

A main motivation for us to build
the proof checker was in fact to contribute to the Flyspeck project, for the
fully formalized proof of the Kepler conjecture. As lemmas for the proof,
hundreds of nonlinear real inequalities need to be verified. Although the
formulas usually contain only around ten variables, they contain a huge number
of nonlinear arithmetic operations and trigonometric functions, and are
mathematically challenging. In the original proof, Hales implemented
procedures that combine linear programming and interval arithmetic to
establishes all these formulas, but the algorithms are hard to be formally
verified. In fact, the formal verification of these formulas is the last main
piece of work needed to complete the full project. The state of the art is
explained in a very recent thesis~\cite{keplerthesis}, reporting the proofs of about 10
inequalities so far. The proofs use formal Taylor series and reliable
numerical methods. The idea for such formal proofs, in short, is to divide the
domains into pieces such that the functions can be formally bounded on each
piece. As is observed, the main difficulty is finding a suitable
partition. Instead of using manual branching, our approach is
essentially using ICP to guide the division of the domain in an efficient way,
such that the interval bounds (the axioms) can be verified more easily.


Without any particular optimization on ICP, we have already observed promising
results. Out of a total number of 505 nonlinear formulas in the Flyspeck project
repository (translated from {\tt HOL} to {\tt smt2} format), we solved
137 of them returning {\sf unsat} with a timeout of 30 minutes and
$\delta=10^{-3}$. Out of these formulas, we applied the proof checking
algorithm, and formally proved 55 of them directly, most of them only takes less
than 1 minutes. The proof traces of these formulas can be very large, for
instance over we proved one with 47k lines in the proof (1MB file). In
Table~\ref{}, we list some of the representative benchmarks to show
scalability.

In the table, the columns denote the following information.


As the proof-generation capacity is for debugging SMT solvers, the proofs have
also been valuable for us to observe bugs during the process. For instance, we
have observed unstable behavior in the trigonometric function evaluation in {\sf
realpaver} (the cosine function around $4\pi/3$). Also, we have observed a
benchmark that are decided as {\sf unsat} on AMD cores, and {\sf $\delta$-sat}
on Intel cores. (Note that both can be correct answers because the two cases
overlap.) However, our proof checker only involves simple operations written in
{\sf OCaml}, and performs the checking completely independent from the solver.
Thus, for the {\sf unsat} answers that we have obtained a proof for, we are
highly confident about the correctness of the proof, which does not involve
possible bugs from {\sf realpaver}.

We regard our work as a first step in a promising approach towards the formal
verification of the nonlinear lemmas in the Flyspeck project. Further work
would involve proof abstractions, local heuristics, and a veritably correct
implementation of our proof checker in HOL.


\section{Conclusion}

We presented our approach for extracting formal proofs from a numerically-driven
decision procedure in the DPLL$\langle$ICP$\rangle$ framework.

We formalized the ICP algorithm, and showed how to construct proof trees from
the unsat answers. We then validated the generated proofs using a stand-alone symbolic proof
checker, which only implements simple rules that are easy to verify,
and interacts with the solver in an abstraction refinement loop to obtain
proof trees of sufficient detail. A main focus for our tool is to prove
nonlinear lemmas in the Flyspeck project, and we have observed promising
experimental results. We believe the approach can be combined with exiting
symbolic approaches, and can be further optimized to solve the challenging
benchmarks in the Flyspeck project.



\bibliographystyle{abbrv}
\bibliography{tau}



\end{document}




\newpage
\section*{Appendix}

\section*{Comparisons}

MetiTarski needs axioms to reduce to polynomials, and then polynomials to be
verifying by outside solvers. Note that the outside solvers are not verified,
and actually hard to be verified. The numerical approach, on the other hand,
can rely on any external solver that are easy to be verified by multiple
solvers, and can be done symbolically. Once it is done symbolically, we obtain
a symbolic proof that does not mention any numerical computations.

The following benchmark can be solved by MetiTarski, which is used in verifying
the aircraft example in~\cite{}.
\begin{example}[Aircraft]
The aircraft formula is
\begin{eqnarray*}
& &\forall t \forall x_1\forall x_2 \forall y_1\forall y_2\forall
d_1\forall d_2\forall e_1\forall e_2. \\
& &0<t<10\wedge x_1<-9 \wedge x_2<-1\wedge
y_1>10 \wedge y_2>10\\
& & \wedge 0.1<d1< 0.15\wedge 0.1<d_2<0.15 \wedge
0.1<e_1<0.15\wedge 0.1<e_2<0.15 \\
\rightarrow & &\Big(
\Big(x_1-y_1-100d_2-100e_2+(100d_2+100e_2)\cos(0.01t)\\
& &\ \ \ + (100d_1
-100e_1)\sin(0.01t)\Big)^2\\
& & \ \ +\Big((x_2-y_2+100d_1+100e_1+(-100d_1-100e_1)\cos(0.
0 1 t ) \\
& &\ \ \  + (100d_2 - 100e_2)\sin(0.01t)\Big)^2 > 2\Big).
\end{eqnarray*}
The formula is solved in MetiTarski in 924
seconds~\cite{}. On comparable machines, dReal solves in 0.01 seconds, also
completely verifies the proof of unsatisfiability in 0.41 seconds, and the
proof itself is ... long~\cite{}.
\end{example}
On the other hand, numerical approach has its inherent limitations. For
instance, for simple formulas in the MetiTarski benchmark set such as the
following, our solver keeps returning $\delta$-sat, and thus do not provide a
proof of unsatisfiability.
\begin{example}

\end{example}



\vspace{-.2cm}
\begin{algorithm}[h!]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{Constraints $c_1,...,c_m$, initial box $B^0
= I^0_1\times \cdots \times I^0_n$, box stack $S=\emptyset$, and precision
$\varepsilon\in \mathbb{Q}^+$.}
\Output{{\sf unsat} and {\sf possibly-sat}.}
\BlankLine
$S.\mathrm{push}(B_0)$\;
\While{$S\neq \emptyset$}{\label{while}
$B\leftarrow S.\mathrm{pop}()$ \;
\While{$\exists 1\leq i \leq m, B\neq \mathrm{Prune}(B,f_i)$}{
%\For{$j\leftarrow 1$ \KwTo $m$}{
        $B\leftarrow\mathrm{Prune}(B, f_i)$ \;
%       \If{$B=\emptyset$}{break\;}
}
\If{$B\neq \emptyset$}
{\eIf{$\exists 1\leq i\leq n, |I_i|\geq \varepsilon$}{$\{B_1,B_2\}\leftarrow
\mathrm{Branch}(B, i)$\;$S.\mathrm{push}(\{B_1,B_2\})$\;}{return {\sf
possibly-sat}\;}}
}
return {\sf unsat}\;
\caption{High-Level ICP$_{\varepsilon}$ (decision version of
Branch-and-Prune)\label{algo_icp}}
\end{algorithm}
\vspace{-.2cm}
